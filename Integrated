# Load Libraries

# python
import os
import re
import spacy
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
from transformers import pipeline as transformers_pipeline
from autocorrect import Speller

# Load spaCy model
nlp = spacy.load('en_core_web_sm')

# Initialize spell checker
spell = Speller()

#####################
# 1. Data Collection
Collect the data from provided documents (for example .docx files) to extract email content for further processing.
#####################
from docx import Document

# Function to extract text from docx files
def extract_text_from_docx(docx_file):
    doc = Document(docx_file)
    full_text = []
    for para in doc.paragraphs:
        full_text.append(para.text)
    return '\n'.join(full_text)

# Example usage
file_path = 'path_to_your_docx_file'
text = extract_text_from_docx(file_path)
print(text)

#######################
# 2. Data Preprocessing
Clean the email text by removing unnecessary information, standardizing formats, and preparing the data for analysis.
#######################


# Function to remove personal information
def remove_personal_info(text):
    doc = nlp(text)
    return ' '.join([token.text for token in doc if not token.ent_type_ in ['PERSON', 'EMAIL', 'PHONE', 'GPE']])

# Function to correct typos
def correct_typos(text):
    return spell(text)

# Function to standardize format
def standardize_format(text):
    return text.lower()

# Preprocess the extracted text
def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text

# Example usage
cleaned_text = preprocess_text(text)
print(cleaned_text)

######################
3. Data Annotation
Label the preprocessed emails with metadata such as structure, tone, and purpose for training the NLP model.
#######################


# Example of adding a manual label (e.g., tone, purpose)
emails = [{'email': cleaned_text, 'category': 'Meeting Request', 'tone': 'Formal'}]

# Display the annotated email
print(emails)

##########################
# 4. Exploratory Data Analysis (EDA)
# Explore patterns, word frequencies, and email structures in the annotated data to guide model selection.
##########################

# Basic Descriptive Statistics
# Calculate number of words and characters per email
emails_df['word_count'] = emails_df['cleaned_email_body'].apply(lambda x: len(word_tokenize(x)))
emails_df['char_count'] = emails_df['cleaned_email_body'].apply(lambda x: len(x))

# Display basic statistics
print(emails_df[['word_count', 'char_count']].describe())
# Sentiment Analysis
from textblob import TextBlob

# Sentiment Analysis
emails_df['sentiment'] = emails_df['cleaned_email_body'].apply(lambda x: TextBlob(x).sentiment.polarity)
emails_df['subjectivity'] = emails_df['cleaned_email_body'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

# Display sentiment distribution
print(emails_df[['sentiment', 'subjectivity']].describe())
# Bigram and Trigram Analysis
from nltk import ngrams

def generate_ngrams(text, n):
    tokens = word_tokenize(text)
    n_grams = list(ngrams(tokens, n))
    return n_grams

# Generate bigrams
emails_df['bigrams'] = emails_df['cleaned_email_body'].apply(lambda x: generate_ngrams(x, 2))

# Display most common bigrams
bigram_freq = Counter([bigram for sublist in emails_df['bigrams'] for bigram in sublist])
print(bigram_freq.most_common(10))
# Word Cloud for Visualization
from wordcloud import WordCloud

# Create a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(tokens))

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
# Named Entity Recognition (NER)
import spacy

# Load SpaCy's pre-trained NER model
nlp = spacy.load('en_core_web_sm')

# Extract named entities from the text
def extract_named_entities(text):
    doc = nlp(text)
    return [(ent.text, ent.label_) for ent in doc.ents]

# Apply NER to the email content
emails_df['entities'] = emails_df['cleaned_email_body'].apply(extract_named_entities)

# Display the extracted entities
print(emails_df[['cleaned_email_body', 'entities']].head())
# Keyword/Topic Extraction
from sklearn.feature_extraction.text import TfidfVectorizer

# Compute TF-IDF for the email dataset
tfidf = TfidfVectorizer(stop_words='english', max_features=100)
tfidf_matrix = tfidf.fit_transform(emails_df['cleaned_email_body'])

# Display top TF-IDF words
feature_names = tfidf.get_feature_names_out()
print("Top 10 TF-IDF words:", feature_names[:10])
# Email Category Distribution
# Assuming the dataset has a 'category' column
category_counts = emails_df['category'].value_counts()

# Plot category distribution
category_counts.plot(kind='bar', title='Email Category Distribution')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
import matplotlib.pyplot as plt

# Download stopwords for tokenization
nltk.download('punkt')
nltk.download('stopwords')

# Tokenize and remove stopwords
def tokenize_and_remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text)
    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]
    return filtered_tokens

# Example usage
tokens = tokenize_and_remove_stopwords(cleaned_text)
word_freq = Counter(tokens)

# Plot word frequencies
common_words = word_freq.most_common(10)
words, counts = zip(*common_words)
plt.bar(words, counts)
plt.title('Top 10 Most Common Words')
plt.show()

###################################
# 5. Model Selection and Design
# Choose an appropriate NLP model Begin by considering pre-trained models that are well-suited for text generation and fine-tuning.and configure it for email template generation.
################################

#################################
# 6. Model Training
# Train the NLP model using the annotated data to generate context-specific email templates.
#############################

#############################
# 7. Implement Reinforcement Learning
# Implement a feedback mechanism to improve the model's performance based on user feedback during email template generation.
##############################

# Load data
data_dir = 'c:\\project1'
documents = []
labels = []

for filename in os.listdir(data_dir):
    if filename.endswith('.txt'):
        with open(os.path.join(data_dir, filename), 'r') as file:
            text = file.read()
            text = remove_personal_info(text)
            text = correct_typos(text)
            text = standardize_format(text)
            documents.append(text)
            # Assuming labels are in the filename, e.g., 'meeting_1.txt'
            labels.append(filename.split('_'))

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=42)

# Create a pipeline for classification
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', MultinomialNB())
])

# Train the model
pipeline.fit(X_train, y_train)

# Predict on the test set
y_pred = pipeline.predict(X_test)

# Print classification report
print(classification_report(y_test, y_pred))

# Plot confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 7))
plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()