# Notedbook 01 '01_data_preprocessing.ipynb'
# Import necessary libraries
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import os

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Load raw email data
def load_emails():
    # Load raw email data from text files
    files = ['/content/drive/My Drive/data/raw/Conference_Meetup.txt', 
             '/content/drive/My Drive/data/raw/Delivering_Annual_Impact_Factor_Report.txt', 
             '/content/drive/My Drive/data/raw/Delivering_Annual_Publishers_Report.txt', 
             '/content/drive/My Drive/data/raw/Delivering_Contractual_Notice.txt',
             '/content/drive/My Drive/data/raw/Delivering_Royalty_Statements.txt',
             '/content/drive/My Drive/data/raw/Editor_Search_Invitation.txt',
             '/content/drive/My Drive/data/raw/Editorial_Board_Recruitment.txt',
             '/content/drive/My Drive/data/raw/Impact_Factor_Initial_Announcement.txt',
             '/content/drive/My Drive/data/raw/Industry_Initiative_Annoucement.txt',
             '/content/drive/My Drive/data/raw/New_Editorship_Offer.txt']
    raw_data_list = []
    for file in files:
        with open(file, 'r') as f:
            text = f.read()
            raw_data_list.append({'text': text})
    
    raw_data = pd.DataFrame(raw_data_list)
    return raw_data

# Example: Clean and tokenize text
stop_words = set(stopwords.words('english'))

def preprocess(text):
    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize
    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove non-alphanumeric tokens and stopwords
    return tokens

def clean_email_data(raw_data):
    # Apply the preprocessing function to the 'text' column and store results in 'clean_text'
    raw_data['clean_text'] = raw_data['text'].apply(preprocess)
    return raw_data

# Load and clean the email data
raw_data = load_emails()
if raw_data.empty:
    print("No data loaded. Please check the file paths.")
else:
    clean_data = clean_email_data(raw_data)
    print(clean_data)

# Notebook 02 '02_exploratory_data_analysis.ipynb'

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Load preprocessed data
cleaned_data = pd.read_csv('data/processed/cleaned_data.csv')

# Generate a word cloud from the cleaned text data
text = ' '.join([' '.join(tokens) for tokens in cleaned_data['clean_text']])
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Plot distribution of email lengths
email_lengths = cleaned_data['clean_text'].apply(len)

plt.figure(figsize=(10, 6))
plt.hist(email_lengths, bins=20, color='blue', edgecolor='black')
plt.title('Distribution of Email Lengths (in tokens)')
plt.xlabel('Number of Tokens')
plt.ylabel('Frequency')
plt.show()

# Notedbook 03 '03_model_training.ipynb'

# Import necessary libraries
import pandas as pd
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load preprocessed data
cleaned_data = pd.read_csv('data/processed/cleaned_data.csv')

# Load GPT-2 model and tokenizer
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Example input (you would use cleaned data for actual training)
sample_text = " ".join(cleaned_data['clean_text'].iloc[0])
inputs = tokenizer(sample_text, return_tensors="pt")

# Train the model (expand this section to include full training loop)
outputs = model.generate(inputs['input_ids'], max_length=100)

# Save the trained model
model.save_pretrained('models/trained_gpt2_model')
tokenizer.save_pretrained('models/trained_gpt2_tokenizer')

print("Model and tokenizer saved in 'models/' directory.")

# Notebook 04 '04_model_evaluation.ipynb'

# Import necessary libraries
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from nltk.translate.bleu_score import sentence_bleu

# Load the trained model and tokenizer
model = GPT2LMHeadModel.from_pretrained('models/trained_gpt2_model')
tokenizer = GPT2Tokenizer.from_pretrained('models/trained_gpt2_tokenizer')

# Example input to generate text
context = "I hope this email finds you well."
input_ids = tokenizer(context, return_tensors="pt")

# Generate email continuation based on the context
generated_email = model.generate(input_ids['input_ids'], max_length=100)

# Decode the generated email
generated_email_text = tokenizer.decode(generated_email[0], skip_special_tokens=True)
print("Generated Email:", generated_email_text)

# Example BLEU score evaluation
reference = [['i', 'hope', 'this', 'email', 'finds', 'you', 'well']]
generated = generated_email_text.lower().split()
bleu_score = sentence_bleu(reference, generated)
print(f"BLEU Score: {bleu_score}")

# Notebook 05 '05_email_template_generation.ipynb'

# Import necessary libraries
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the trained model and tokenizer
model = GPT2LMHeadModel.from_pretrained('models/trained_gpt2_model')
tokenizer = GPT2Tokenizer.from_pretrained('models/trained_gpt2_tokenizer')

# Example context for email generation
context = "I hope this email finds you well. I'm writing to follow up on our previous discussion regarding the project."

# Tokenize input context
input_ids = tokenizer(context, return_tensors="pt")

# Generate email template based on the context
generated_email = model.generate(input_ids['input_ids'], max_length=100)

# Decode and display the generated email
generated_email_text = tokenizer.decode(generated_email[0], skip_special_tokens=True)
print("Generated Email Template:", generated_email_text)

# Notebook 06 '06_outlook_integration.ipynb'

# Import necessary libraries
import win32com.client as win32

# Example generated email text
generated_email_text = "Dear John, I hope this email finds you well. This is a test email generated by the AI system."

# Connect to Outlook
outlook = win32.Dispatch('outlook.application')

# Create a new mail item
mail = outlook.CreateItem(0)  # 0 means it's a mail item

# Set email parameters
mail.Subject = "Generated Email from AI"
mail.Body = generated_email_text
mail.To = "recipient@example.com"

# Send the email
mail.Send()

print("Email sent successfully!")

Notebook 'Email_Template_Generation_Project.ipynb'
# 1. Data Preprocessing

# Import necessary libraries
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import os

# Download stopwords (ensure that you have nltk installed and the stopwords dataset)
import nltk
# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Load raw email data
def load_emails():
    # Load raw email data from text files
    files = ['/C:/Users/humphrys/Documents/ForkNatNew/data/raw/Conference_Meetup.txt', 
             '/C:/Users/humphrys/Documents/ForkNatNew/data/raw/Delivering_Annual_Impact_Factor_Report.txt', 
             '/C:/Users/humphrys/Documents/ForkNatNew/data/raw/Delivering_Annual_Publishers_Report.txt', 
             '/C:/Users/humphrys/Documents/ForkNatNew/data/raw/Delivering_Contractual_Notice.txt',
             '/C:/Users/humphrys/Documents/ForkNatNew/data/raw/Delivering_Royalty_Statements.txt',
             '/C:/Users/humphrys/Documents/ForkNatNew/data/raw//Editor_Search_Invitation.txt',
             '/C:/Users/humphrys/Documents/ForkNatNew/data/raw/Editorial_Board_Recruitment.txt',
             '/C:/Users/humphrys/Documents/ForkNatNew/data/raw/Impact_Factor_Initial_Announcement.txt',
             '/C:/Users/humphrys/Documents/ForkNatNew/data/raw/Industry_Initiative_Annoucement.txt',
             '/C:/Users/humphrys/Documents/ForkNatNew/data/raw/New_Editorship_Offer.txt']
    raw_data_list = []
    for file in files:
        with open(file, 'r') as f:
            text = f.read()
            raw_data_list.append({'text': text})
    
    raw_data = pd.DataFrame(raw_data_list)
    return raw_data

# Example: Clean and tokenize text
stop_words = set(stopwords.words('english'))

def preprocess(text):
    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize
    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove non-alphanumeric tokens and stopwords
    return tokens

def clean_email_data(raw_data):
    # Apply the preprocessing function to the 'text' column and store results in 'clean_text'
    raw_data['clean_text'] = raw_data['text'].apply(preprocess)
    return raw_data

# Example: Clean and tokenize text (expand as necessary)
stop_words = set(stopwords.words('english'))

def preprocess(text):
    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize
    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove non-alphanumeric tokens and stopwords
    return tokens

# Apply preprocessing to each email
raw_data['clean_text'] = raw_data['text'].apply(preprocess)

# Display the first few rows of the cleaned data
raw_data.head()

# 2. Exploratory Data Analysis (EDA)

# Import visualization libraries
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Join the clean text into a single string for analysis
text = ' '.join([' '.join(tokens) for tokens in raw_data['clean_text']])

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Plot distribution of email lengths
email_lengths = raw_data['clean_text'].apply(len)

plt.figure(figsize=(10, 6))
plt.hist(email_lengths, bins=20, color='blue', edgecolor='black')
plt.title('Distribution of Email Lengths (in tokens)')
plt.xlabel('Number of Tokens')
plt.ylabel('Frequency')
plt.show()

# 3. Model Training

# Import necessary machine learning libraries
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load pre-trained GPT-2 model and tokenizer
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Prepare example data (you would use your dataset for training)
sample_email = "Hello, I wanted to follow up on our meeting."
inputs = tokenizer(sample_email, return_tensors="pt")

# Generate a response using the pre-trained model
outputs = model.generate(inputs['input_ids'], max_length=50)

# Decode and print the generated text
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated Email:", generated_text)

# 4. Model Evaluation

# Example of evaluating model accuracy
# Since this is a generative task, evaluation could involve metrics like BLEU or ROUGE scores for text generation.

# Import BLEU score for evaluation (if using NLTK)
from nltk.translate.bleu_score import sentence_bleu

# Reference sentences and a generated sentence example
reference = [['hello', 'this', 'is', 'a', 'test', 'email']]
generated = ['hello', 'this', 'is', 'an', 'example']

# Calculate BLEU score
bleu_score = sentence_bleu(reference, generated)
print(f"BLEU Score: {bleu_score}")

# You can expand this to compare model outputs to a reference dataset and calculate overall performance.

# 5. Email Template Generation

# Use the trained model to generate email templates based on input

# Example: Generate an email template based on the context
context = "I hope this email finds you well. I'm writing to follow up on our previous discussion regarding the project."

# Tokenize input context
input_ids = tokenizer(context, return_tensors="pt")

# Generate email continuation based on the context
generated_email = model.generate(input_ids['input_ids'], max_length=100)

# Decode and print the generated email
generated_email_text = tokenizer.decode(generated_email[0], skip_special_tokens=True)
print("Generated Email Template:", generated_email_text)

# 6. Outlook Integration

# Example code for integrating with Outlook using pywin32 (Windows only)

import win32com.client as win32

# Connect to the Outlook application
outlook = win32.Dispatch('outlook.application')

# Create a new email item
mail = outlook.CreateItem(0)  # 0 indicates mail item

# Set email parameters
mail.Subject = "Test Email from Python"
mail.Body = "This is a test email generated by Python using pywin32."
mail.To = "recipient@example.com"

# Send the email
mail.Send()

print("Email sent successfully!")
